{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-14T20:43:53.842497Z",
     "start_time": "2025-04-14T20:43:45.326426Z"
    }
   },
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client(project=\"virtualization-and-cloud\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T20:44:23.415920Z",
     "start_time": "2025-04-14T20:44:01.838174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.driver.memory\", \"16384m\") \\\n",
    "    .config(\"spark.executor.memory\", \"16384m\") \\\n",
    "    .getOrCreate()\n"
   ],
   "id": "f62fe1024cd2d2f3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T20:47:21.772244Z",
     "start_time": "2025-04-14T20:46:30.998745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"\"\"\n",
    "SELECT userId, movieId, rating\n",
    "FROM `virtualization-and-cloud.movies.ratings`\n",
    "\"\"\"\n",
    "ratings_df = client.query(query).to_dataframe()"
   ],
   "id": "b245a208db0d7d80",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:04:35.976016Z",
     "start_time": "2025-04-14T20:48:41.315734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to Spark DataFrame\n",
    "ratings_sdf = spark.createDataFrame(ratings_df)"
   ],
   "id": "6f1e1c8b7f8c6e4b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:04:55.690864Z",
     "start_time": "2025-04-14T21:04:55.651313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Convert userId and movieId to integer if needed\n",
    "from pyspark.sql.functions import col\n",
    "ratings_sdf = ratings_sdf.select(\n",
    "    col(\"userId\").cast(\"int\"),\n",
    "    col(\"movieId\").cast(\"int\"),\n",
    "    col(\"rating\").cast(\"float\")\n",
    ")"
   ],
   "id": "e8c006b67d48e7e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:49:05.034143Z",
     "start_time": "2025-04-14T13:49:04.823371Z"
    }
   },
   "cell_type": "code",
   "source": "(training, test) = ratings_sdf.randomSplit([0.8, 0.2], seed=42)",
   "id": "f6393a503b3fa6da",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:58:53.253580Z",
     "start_time": "2025-04-14T13:58:53.210735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True,\n",
    "    implicitPrefs=False,\n",
    ")"
   ],
   "id": "7d3e87a59f497d22",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:58:57.557917Z",
     "start_time": "2025-04-14T13:58:57.549166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 20, 30]) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(als.maxIter, [5, 10]) \\\n",
    "    .build()"
   ],
   "id": "e6f63dd0ffc4593",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:58:59.213115Z",
     "start_time": "2025-04-14T13:58:59.189084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")"
   ],
   "id": "aa61b3b1c9b4d673",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:59:00.878275Z",
     "start_time": "2025-04-14T13:59:00.859673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cv = CrossValidator(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,         # 3-fold cross-validation\n",
    "    parallelism=4       # Speeds up grid search using parallelism\n",
    ")"
   ],
   "id": "c0f57b70e1e2b94f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T16:04:04.426273Z",
     "start_time": "2025-04-14T14:34:34.458100Z"
    }
   },
   "cell_type": "code",
   "source": "cv_model = cv.fit(training)",
   "id": "3e6201100f77fe85",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T16:06:30.324295Z",
     "start_time": "2025-04-14T16:06:30.311830Z"
    }
   },
   "cell_type": "code",
   "source": "best_model = cv_model.bestModel",
   "id": "c2037650e865248a",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T16:06:32.452354Z",
     "start_time": "2025-04-14T16:06:32.412481Z"
    }
   },
   "cell_type": "code",
   "source": "predictions = best_model.transform(test)",
   "id": "b74521781c22f4dd",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T16:04:45.298020Z",
     "start_time": "2025-04-14T16:04:41.719878Z"
    }
   },
   "cell_type": "code",
   "source": "cv_model.bestModel.save(\"als_best_model\")",
   "id": "4d6b5f386e4c45ab",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T20:44:56.487437Z",
     "start_time": "2025-04-14T20:44:45.570241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "loaded_model = ALSModel.load(\"als_best_model\")"
   ],
   "id": "603908c1b4b44f6a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:11:12.648203Z",
     "start_time": "2025-04-14T21:11:12.418851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k = 10\n",
    "user_recommendations = loaded_model.recommendForAllUsers(k)"
   ],
   "id": "3a49923b63f6630f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:11:21.209683Z",
     "start_time": "2025-04-14T21:11:21.167247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import collect_set\n",
    "\n",
    "# Assume ratings_df contains userId, movieId, rating\n",
    "# Define \"relevant\" items as those rated >= 4.0 (you can tweak this threshold)\n",
    "relevant_ratings = ratings_sdf.filter(col(\"rating\") >= 4.0) \\\n",
    "                              .groupBy(\"userId\") \\\n",
    "                              .agg(collect_set(\"movieId\").alias(\"relevant_movies\"))"
   ],
   "id": "b03a1a83780b0025",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:25:17.868126Z",
     "start_time": "2025-04-14T21:25:17.839885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Join recommendations with ground truth\n",
    "recommendations_with_truth = user_recommendations.join(relevant_ratings, on=\"userId\")\n",
    "\n",
    "# Convert recommendation list to just movieIds\n",
    "recommendations_with_truth = recommendations_with_truth.withColumn(\n",
    "    \"recommended_movies\",\n",
    "    expr(\"transform(recommendations, x -> x.movieId)\")\n",
    ")"
   ],
   "id": "45d62fa4a87515f4",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:16:15.284598Z",
     "start_time": "2025-04-14T21:16:15.257441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k = 10  # or whatever value you want for @K\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import math\n",
    "\n",
    "def precision_at_k(recommended, relevant):\n",
    "    if not relevant: return 0.0\n",
    "    recommended_k = recommended[:k]\n",
    "    hits = len(set(recommended_k) & set(relevant))\n",
    "    return hits / float(k)\n",
    "\n",
    "def recall_at_k(recommended, relevant):\n",
    "    if not relevant: return 0.0\n",
    "    recommended_k = recommended[:k]\n",
    "    hits = len(set(recommended_k) & set(relevant))\n",
    "    return hits / float(len(relevant))\n",
    "\n",
    "def make_apk_udf(k):\n",
    "    def apk(recommended, relevant):\n",
    "        if not relevant: return 0.0\n",
    "        score = 0.0\n",
    "        hits = 0\n",
    "        for i, p in enumerate(recommended[:k]):\n",
    "            if p in relevant:\n",
    "                hits += 1\n",
    "                score += hits / (i + 1.0)\n",
    "        return score / min(len(relevant), k)\n",
    "    return udf(apk, FloatType())\n",
    "\n",
    "def make_ndcg_udf(k):\n",
    "    def ndcg(recommended, relevant):\n",
    "        if not relevant: return 0.0\n",
    "        dcg = 0.0\n",
    "        for i, p in enumerate(recommended[:k]):\n",
    "            if p in relevant:\n",
    "                dcg += 1.0 / math.log2(i + 2)\n",
    "        idcg = sum(1.0 / math.log2(i + 2) for i in range(min(len(relevant), k)))\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "    return udf(ndcg, FloatType())\n",
    "\n",
    "# Register UDFs with fixed k\n",
    "precision_udf = udf(precision_at_k, FloatType())\n",
    "recall_udf = udf(recall_at_k, FloatType())\n",
    "apk_udf = make_apk_udf(k)\n",
    "ndcg_udf = make_ndcg_udf(k)"
   ],
   "id": "25b05a4de80849a7",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:20:06.004316Z",
     "start_time": "2025-04-14T21:16:18.601415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metrics_df = recommendations_with_truth \\\n",
    "    .withColumn(\"precision\", precision_udf(\"recommended_movies\", \"relevant_movies\")) \\\n",
    "    .withColumn(\"recall\", recall_udf(\"recommended_movies\", \"relevant_movies\")) \\\n",
    "    .withColumn(\"apk\", apk_udf(\"recommended_movies\", \"relevant_movies\")) \\\n",
    "    .withColumn(\"ndcg\", ndcg_udf(\"recommended_movies\", \"relevant_movies\"))\n",
    "\n",
    "final_metrics = metrics_df.select(\"precision\", \"recall\", \"apk\", \"ndcg\").agg(\n",
    "    {\"precision\": \"avg\", \"recall\": \"avg\", \"apk\": \"avg\", \"ndcg\": \"avg\"}\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Precision@{k}: {final_metrics['avg(precision)']:.4f}\")\n",
    "print(f\"Recall@{k}: {final_metrics['avg(recall)']:.4f}\")\n",
    "print(f\"MAP@{k}: {final_metrics['avg(apk)']:.4f}\")\n",
    "print(f\"NDCG@{k}: {final_metrics['avg(ndcg)']:.4f}\")"
   ],
   "id": "619c6e49c9cd9eaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.0001\n",
      "Recall@10: 0.0003\n",
      "MAP@10: 0.0002\n",
      "NDCG@10: 0.0003\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:26:30.453118Z",
     "start_time": "2025-04-14T21:26:30.443025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the columns in both DataFrames\n",
    "print(\"Columns in user_recommendations:\", user_recommendations.columns)\n",
    "print(\"Columns in relevant_ratings:\", relevant_ratings.columns)"
   ],
   "id": "d526eca63284f229",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in user_recommendations: ['userId', 'recommendations']\n",
      "Columns in relevant_ratings: ['userId', 'relevant_movies']\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T16:07:21.474342Z",
     "start_time": "2025-04-14T16:06:34.384228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import when\n",
    "predictions = predictions.withColumn(\n",
    "    \"prediction\",\n",
    "    when(col(\"prediction\") > 5, 5).when(col(\"prediction\") < 1, 1).otherwise(col(\"prediction\"))\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Best model RMSE after tuning: {rmse:.4f}\")"
   ],
   "id": "41ab20c23f413dcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model RMSE after tuning: 0.8138\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T16:07:21.567134Z",
     "start_time": "2025-04-14T16:07:21.539757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_rank = cv_model.bestModel._java_obj.parent().getRank()\n",
    "best_regParam = cv_model.bestModel._java_obj.parent().getRegParam()\n",
    "best_maxIter = cv_model.bestModel._java_obj.parent().getMaxIter()\n",
    "\n",
    "print(f\"Best Rank: {best_rank}\")\n",
    "print(f\"Best RegParam: {best_regParam}\")\n",
    "print(f\"Best MaxIter: {best_maxIter}\")"
   ],
   "id": "fe1e2474161ef110",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Rank: 30\n",
      "Best RegParam: 0.1\n",
      "Best MaxIter: 10\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T18:52:39.979501Z",
     "start_time": "2025-04-13T18:52:39.956817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "best_params = {\n",
    "    \"rank\": 30,\n",
    "    \"regParam\": 0.1,\n",
    "    \"maxIter\": 10\n",
    "}\n",
    "\n",
    "with open(\"best_als_params.json\", \"w\") as f:\n",
    "    json.dump(best_params, f)"
   ],
   "id": "cf87ef25ec64fb38",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T16:07:21.784029Z",
     "start_time": "2025-04-14T16:07:21.602824Z"
    }
   },
   "cell_type": "code",
   "source": "user_recs = best_model.recommendForAllUsers(100)",
   "id": "adb4be8a9872dbde",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T16:07:21.972119Z",
     "start_time": "2025-04-14T16:07:21.850248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import explode, col, current_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Flatten the nested recommendations\n",
    "flattened_recs = user_recs.select(\n",
    "    col(\"userId\"),\n",
    "    explode(col(\"recommendations\")).alias(\"rec\")\n",
    ").select(\n",
    "    col(\"userId\"),\n",
    "    col(\"rec.movieId\").alias(\"movieId\"),\n",
    "    col(\"rec.rating\").alias(\"predicted_rating\")\n",
    ")\n",
    "\n",
    "# Add ranking per user\n",
    "window = Window.partitionBy(\"userId\").orderBy(col(\"predicted_rating\").desc())\n",
    "flattened_recs = flattened_recs.withColumn(\"rank\", row_number().over(window))\n",
    "\n",
    "# Add timestamp column\n",
    "flattened_recs = flattened_recs.withColumn(\"generated_at\", current_timestamp())"
   ],
   "id": "ba55da0f787f7892",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T17:35:50.292523Z",
     "start_time": "2025-04-14T17:35:48.865464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"flattened_recs_parquet\")"
   ],
   "id": "98c696ec50ba3d7",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T17:47:36.029747Z",
     "start_time": "2025-04-14T17:47:36.022753Z"
    }
   },
   "cell_type": "code",
   "source": "table_ref = client.dataset(\"movies\").table(\"recommendationsALS\")",
   "id": "ec8fb450ba823487",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T17:50:52.312775Z",
     "start_time": "2025-04-14T17:49:41.298449Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=virtualization-and-cloud, location=US, id=a3394507-fcaa-4c11-a9bc-130f0568a330>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55,
   "source": "client.load_table_from_dataframe(df, table_ref, job_config=bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE)).result()",
   "id": "2b34a1af37c2be78"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bigdata)",
   "language": "python",
   "name": "bigdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
